Microsoft and Atom Computing combine for quantum error correction demo
New work provides a good view of where the field currently stands. In September, Microsoft made an unusual combination of announcements.  It demonstrated progress with quantum error correction, something that will be needed for the technology to move much beyond the interesting demo phase, using hardware from a quantum computing startup called Quantinuum.  At the same time, however, the company also announced that it was forming a partnership with a different startup, Atom Computing, which uses a different technology to make qubits available for computations. 

Given that, it was probably inevitable that the folks in Redmond, Washington, would want to show that similar error correction techniques would also work with Atom Computing's hardware.  It didn't take long, as the two companies are releasing a draft manuscript describing their work on error correction today.  The paper serves as both a good summary of where things currently stand in the world of error correction, as well as a good look at some of the distinct features of computation using neutral atoms. 

Atoms and errors
While we have various technologies that provide a way of storing and manipulating bits of quantum information, none of them can be operated error-free.  At present, errors make it difficult to perform even the simplest computations that are clearly beyond the capabilities of classical computers.  More sophisticated algorithms would inevitably encounter an error before they could be completed, a situation that would remain true even if we could somehow improve the hardware error rates of qubits by a factor of 1,000—something we're unlikely to ever be able to do. 

The solution to this is to use what are called logical qubits, which distribute quantum information across multiple hardware qubits and allow the detection and correction of errors when they occur.  Since multiple qubits get linked together to operate as a single logical unit, the hardware error rate still matters.  If it's too high, then adding more hardware qubits just means that errors will pop up faster than they can possibly be corrected. 
We're now at the point where, for a number of technologies, hardware error rates have passed the break-even point, and adding more hardware qubits can lower the error rate of a logical qubit based on them.  This was demonstrated using neutral atom qubits by an academic lab at Harvard University about a year ago.  The new manuscript demonstrates that it also works on a commercial machine from Atom Computing. 

Neutral atoms, which can be held in place using a lattice of laser light, have a number of distinct advantages when it comes to quantum computing.  Every single atom will behave identically, meaning that you don't have to manage the device-to-device variability that's inevitable with fabricated electronic qubits.  Atoms can also be moved around, allowing any atom to be entangled with any other.  This any-to-any connectivity can enable more efficient algorithms and error-correction schemes.  The quantum information is typically stored in the spin of the atom's nucleus, which is shielded from environmental influences by the cloud of electrons that surround it, making them relatively long-lived qubits. 

Operations, including gates and readout, are performed using lasers.  The way the physics works, the spacing of the atoms determines how the laser affects them.  If two atoms are a critical distance apart, the laser can perform a single operation, called a two-qubit gate, that affects both of their states.  Anywhere outside this distance, and a laser only affects each atom individually.  This allows a fine control over gate operations. 

That said, operations are relatively slow compared to some electronic qubits, and atoms can occasionally be lost entirely.  The optical traps that hold atoms in place are also contingent upon the atom being in its ground state; if any atom ends up stuck in a different state, it will be able to drift off and be lost.  This is actually somewhat useful, in that it converts an unexpected state into a clear error. The machine used in the new demonstration hosts 256 of these neutral atoms.  Atom Computing has them arranged in sets of parallel rows, with space in between to let the atoms be shuffled around.  For single-qubit gates, it's possible to shine a laser across the rows, causing every atom it touches to undergo that operation.  For two-qubit gates, pairs of atoms get moved to the end of the row and moved a specific distance apart, at which point a laser will cause the gate to be performed on every pair present. 

Atom's hardware also allows a constant supply of new atoms to be brought in to replace any that are lost.  It's also possible to image the atom array in between operations to determine whether any atoms have been lost and if any are in the wrong state. 

It’s only logical
As a general rule, the more hardware qubits you dedicate to each logical qubit, the more simultaneous errors you can identify.  This identification can enable two ways of handling the error.  In the first, you simply discard any calculation with an error and start over.  In the second, you can use information about the error to try to fix it, although the repair involves additional operations that can potentially trigger a separate error. 

For this work, the Microsoft/Atom team used relatively small logical qubits (meaning they used very few hardware qubits), which meant they could fit more of them within 256 total hardware qubits the machine made available.  They also checked the error rate of both error detection with discard and error detection with correction. 

The research team did two main demonstrations.  One was placing 24 of these logical qubits into what's called a cat state, named after Schrödinger's hypothetical feline.  This is when a quantum object simultaneously has non-zero probability of being in two mutually exclusive states.  In this case, the researchers placed 24 logical qubits in an entangled cat state, the largest ensemble of this sort yet created.  Separately, they implemented what's called the Bernstein-Vazirani algorithm.  The classical version of this algorithm requires individual queries to identify each bit in a string of them; the quantum version obtains the entire string with a single query, so is a notable case of something where a quantum speedup is possible. Both of these showed a similar pattern.  When done directly on the hardware, with each qubit being a single atom, there was an appreciable error rate.  By detecting errors and discarding those calculations where they occurred, it was possible to significantly improve the error rate of the remaining calculations.  Note that this doesn't eliminate errors, as it's possible for multiple errors to occur simultaneously, altering the value of the qubit without leaving an indication that can be spotted with these small logical qubits. 

Discarding has its limits; as calculations become increasingly complex, involving more qubits or operations, it will inevitably mean every calculation will have an error, so you'd end up wanting to discard everything.  Which is why we'll ultimately need to correct the errors. 

In these experiments, however, the process of correcting the error—taking an entirely new atom and setting it into the appropriate state—was also error-prone.  So, while it could be done, it ended up having an overall error rate that was intermediate between the approach of catching and discarding errors and the rate when operations were done directly on the hardware. 

In the end, the current hardware has an error rate that's good enough that error correction actually improves the probability that a set of operations can be performed without producing an error.  But not good enough that we can perform the sort of complex operations that would lead quantum computers to have an advantage in useful calculations.  And that's not just true for Atom's hardware; similar things can be said for other error-correction demonstrations done on different machines. 

There are two ways to go beyond these current limits.  One is simply to improve the error rates of the hardware qubits further, as fewer total errors make it more likely that we can catch and correct them.  The second is to increase the qubit counts so that we can host larger, more robust logical qubits.  We're obviously going to need to do both, and Atom's partnership with Microsoft was formed in the hope that it will help both companies get there faster. Google gets an error-corrected quantum bit to be stable for an hour
Using almost the entire chip for a logical qubit provides long-term stability. On Monday, Nature released a paper from Google's quantum computing team that provides a key demonstration of the potential of quantum error correction.  Thanks to an improved processor, Google's team found that increasing the number of hardware qubits dedicated to an error-corrected logical qubit led to an exponential increase in performance.  By the time the entire 105-qubit processor was dedicated to hosting a single error-corrected qubit, the system was stable for an average of an hour. 

In fact, Google told Ars that errors on this single logical qubit were rare enough that it was difficult to study them.  The work provides a significant validation that quantum error correction is likely to be capable of supporting the execution of complex algorithms that might require hours to execute. 

A new fab
Google is making a number of announcements in association with the paper's release (an earlier version of the paper has been up on the arXiv since August).  One of those is that the company is committed enough to its quantum computing efforts that it has built its own fabrication facility for its superconducting processors. 

"In the past, all the Sycamore devices that you've heard about were fabricated in a shared university clean room space next to graduate students and people doing kinds of crazy stuff," Google's Julian Kelly said.  "And we've made this really significant investment in bringing this new facility online, hiring staff, filling it with tools, transferring their process over.  And that enables us to have significantly more process control and dedicated tooling. "

That's likely to be a critical step for the company, as the ability to fabricate smaller test devices can allow the exploration of lots of ideas on how to structure the hardware to limit the impact of noise.  The first publicly announced product of this lab is the Willow processor, Google's second design, which ups its qubit count to 105.  Kelly said one of the changes that came with Willow actually involved making the individual pieces of the qubit larger, which makes them somewhat less susceptible to the influence of noise. All of that led to a lower error rate, which was critical for the work done in the new paper.  This was demonstrated by running Google's favorite benchmark, one that it acknowledges is contrived in a way to make quantum computing look as good as possible.  Still, people have figured out how to make algorithm improvements for classical computers that have kept them mostly competitive.  But, with all the improvements, Google expects that the quantum hardware has moved firmly into the lead.  "We think that the classical side will never outperform quantum in this benchmark because we're now looking at something on our new chip that takes under five minutes, would take 1025 years, which is way longer than the age of the Universe," Kelly said. 

Building logical qubits
The work focuses on the behavior of logical qubits, in which a collection of individual hardware qubits are grouped together in a way that enables errors to be detected and corrected.  These are going to be essential for running any complex algorithms, since the hardware itself experiences errors often enough to make some inevitable during any complex calculations. 

This naturally creates a key milestone.  You can get better error correction by adding more hardware qubits to each logical qubit.  If each of those hardware qubits produces errors at a sufficient rate, however, then you'll experience errors faster than you can correct for them.  You need to get hardware qubits of a sufficient quality before you start benefitting from larger logical qubits.  Google's earlier hardware had made it past that milestone, but only barely.  Adding more hardware qubits to each logical qubit only made for a marginal improvement. 

That's no longer the case.  Google's processors have the hardware qubits laid out on a square grid, with each connected to its nearest neighbors (typically four except at the edges of the grid).  And there's a specific error correction code structure, called the surface code, that fits neatly into this grid.  And you can use surface codes of different sizes by using progressively more of the grid.  The size of the grid being used is measured by a term called distance, with larger distance meaning a bigger logical qubit, and thus better error correction. (In addition to a standard surface code, Google includes a few qubits that handle a phenomenon called "leakage," where a qubit ends up in a higher-energy state, instead of the two low-energy states defined as zero and one. )

The key result is that going from a distance of three to a distance of five more than doubled the ability of the system to catch and correct errors.  Going from a distance of five to a distance of seven doubled it again.  Which shows that the hardware qubits have reached a sufficient quality that putting more of them into a logical qubit has an exponential effect. 

"As we increase the grid from three by three to five by five to seven by seven, the error rate is going down by a factor of two each time," said Google's Michael Newman.  "And that's that exponential error suppression that we want. "

Going big
The second thing they demonstrated is that, if you make the largest logical qubit that the hardware can support, with a distance of 15, it's possible to hang onto the quantum information for an average of an hour.  This is striking because Google's earlier work had found that its processors experience widespread simultaneous errors that the team ascribed to cosmic ray impacts.  (IBM, however, has indicated it doesn't see anything similar, so it's not clear whether this diagnosis is correct. ) Those happened every 10 seconds or so.  But this work shows that a sufficiently large error code can correct for these events, whatever their cause. 

That said, these qubits don't survive indefinitely.  One of them seems to be a localized temporary increase in errors.  The second, more difficult to deal with problem involves a widespread spike in error detection affecting an area that includes roughly 30 qubits.  At this point, however, Google has only seen six of these events, so they told Ars that it's difficult to really characterize them.  "It's so rare it actually starts to become a bit challenging to study because you have to gain a lot of statistics to even see those events at all," said Kelly. Beyond the relative durability of these logical qubits, the paper notes another advantage to going with larger code distances: it enhances the impact of further hardware improvements.  Google estimates that at a distance of 15, improving hardware performance by a factor of two would drop errors in the logical qubit by a factor of 250.  At a distance of 27, the same hardware improvement would lead to an improvement of over 10,000 in the logical qubit's performance. 

Note that none of this will ever get the error rate to zero.  Instead, we just need to get the error rate to a level where an error is unlikely for a given calculation (more complex calculations will require a lower error rate).  "It's worth understanding that there's always going to be some type of error floor and you just have to push it low enough to the point where it practically is irrelevant," Kelly said.  "So for example, we could get hit by an asteroid and the entire Earth could explode and that would be a correlated error that our quantum computer is not currently built to be robust to. "

Obviously, a lot of additional work will need to be done to both make logical qubits like this survive for even longer, and to ensure we have the hardware to host enough logical qubits to perform calculations.  But the exponential improvements here, to Google, suggest that there's nothing obvious standing in the way of that.  "We woke up one morning and we kind of got these results and we were like, wow, this is going to work," Newman said.  "This is really it. "

Google’s DeepMind tackles weather forecasting, with great performance
Needs just eight minutes on one processor to do a single 15-day forecast. By some measures, AI systems are now competitive with traditional computing methods for generating weather forecasts.  Because their training penalizes errors, however, the forecasts tend to get "blurry"—as you move further ahead in time, the models make fewer specific predictions since those are more likely to be wrong.  As a result, you start to see things like storm tracks broadening and the storms themselves losing clearly defined edges. 

But using AI is still extremely tempting because the alternative is a computational atmospheric circulation model, which is extremely compute-intensive.  Still, it's highly successful, with the ensemble model from the European Centre for Medium-Range Weather Forecasts considered the best in class. 

In a paper being released today, Google's DeepMind claims its new AI system manages to outperform the European model on forecasts out to at least a week and often beyond.  DeepMind's system, called GenCast, merges some computational approaches used by atmospheric scientists with a diffusion model, commonly used in generative AI.  The result is a system that maintains high resolution while cutting the computational cost significantly. 

Ensemble forecasting
Traditional computational methods have two main advantages over AI systems.  The first is that they're directly based on atmospheric physics, incorporating the rules we know govern the behavior of our actual weather, and they calculate some of the details in a way that's directly informed by empirical data.  They're also run as ensembles, meaning that multiple instances of the model are run.  Due to the chaotic nature of the weather, these different runs will gradually diverge, providing a measure of the uncertainty of the forecast. 

At least one attempt has been made to merge some of the aspects of traditional weather models with AI systems.  An internal Google project used a traditional atmospheric circulation model that divided the Earth's surface into a grid of cells but used an AI to predict the behavior of each cell.  This provided much better computational performance, but at the expense of relatively large grid cells, which resulted in relatively low resolution. 

For its take on AI weather predictions, DeepMind decided to skip the physics and instead adopt the ability to run an ensemble. 

Gen Cast is based on diffusion models, which have a key feature that's useful here.  In essence, these models are trained by starting them with a mixture of an original—image, text, weather pattern—and then a variation where noise is injected.  The system is supposed to create a variation of the noisy version that is closer to the original.  Once trained, it can be fed pure noise and evolve the noise to be closer to whatever it's targeting. 

In this case, the target is realistic weather data, and the system takes an input of pure noise and evolves it based on the atmosphere's current state and its recent history.  For longer-range forecasts, the "history" includes both the actual data and the predicted data from earlier forecasts.  The system moves forward in 12-hour steps, so the forecast for day three will incorporate the starting conditions, the earlier history, and the two forecasts from days one and two. 

This is useful for creating an ensemble forecast because you can feed it different patterns of noise as input, and each will produce a slightly different output of weather data.  This serves the same purpose it does in a traditional weather model: providing a measure of the uncertainty for the forecast. 

For each grid square, GenCast works with six weather measures at the surface, along with six that track the state of the atmosphere and 13 different altitudes at which it estimates the air pressure.  Each of these grid squares is 0. 2 degrees on a side, a higher resolution than the European model uses for its forecasts.  Despite that resolution, DeepMind estimates that a single instance (meaning not a full ensemble) can be run out to 15 days on one of Google's tensor processing systems in just eight minutes. 

It's possible to make an ensemble forecast by running multiple versions of this in parallel and then integrating the results.  Given the amount of hardware Google has at its disposal, the whole process from start to finish is likely to take less than 20 minutes.  The source and training data will be placed on the GitHub page for DeepMind's GraphCast project.  Given the relatively low computational requirements, we can probably expect individual academic research teams to start experimenting with it. 

Measures of success
DeepMind reports that GenCast dramatically outperforms the best traditional forecasting model.  Using a standard benchmark in the field, DeepMind found that GenCast was more accurate than the European model on 97 percent of the tests it used, which checked different output values at different times in the future.  In addition, the confidence values, based on the uncertainty obtained from the ensemble, were generally reasonable. 

Past AI weather forecasters, having been trained on real-world data, are generally not great at handling extreme weather since it shows up so rarely in the training set.  But GenCast did quite well, often outperforming the European model in things like abnormally high and low temperatures and air pressure (one percent frequency or less, including at the 0. 01 percentile). 

DeepMind also went beyond standard tests to determine whether GenCast might be useful.  This research included projecting the tracks of tropical cyclones, an important job for forecasting models.  For the first four days, GenCast was significantly more accurate than the European model, and it maintained its lead out to about a week. 

One of DeepMind's most interesting tests was checking the global forecast of wind power output based on information from the Global Powerplant Database.  This involved using it to forecast wind speeds at 10 meters above the surface (which is actually lower than where most turbines reside but is the best approximation possible) and then using that number to figure out how much power would be generated.  The system beat the traditional weather model by 20 percent for the first two days and stayed in front with a declining lead out to a week. 

The researchers don't spend much time examining why performance seems to decline gradually for about a week.  Ideally, more details about GenCast's limitations would help inform further improvements, so the researchers are likely thinking about it.  In any case, today's paper marks the second case where taking something akin to a hybrid approach—mixing aspects of traditional forecast systems with AI—has been reported to improve forecasts.  And both those cases took very different approaches, raising the prospect that it will be possible to combine some of their features. 

Qubit that makes most errors obvious now available to customers
Can a small machine that makes error correction easier upend the market?We're nearing the end of the year, and there are typically a flood of announcements regarding quantum computers around now, in part because some companies want to live up to promised schedules.  Most of these involve evolutionary improvements on previous generations of hardware.  But this year, we have something new: the first company to market with a new qubit technology. 

The technology is called a dual-rail qubit, and it is intended to make the most common form of error trivially easy to detect in hardware, thus making error correction far more efficient.  And, while tech giant Amazon has been experimenting with them, a startup called Quantum Circuits is the first to give the public access to dual-rail qubits via a cloud service. 

While the tech is interesting on its own, it also provides us with a window into how the field as a whole is thinking about getting error-corrected quantum computing to work. 

What’s a dual-rail qubit?
Dual-rail qubits are variants of the hardware used in transmons, the qubits favored by companies like Google and IBM.  The basic hardware unit links a loop of superconducting wire to a tiny cavity that allows microwave photons to resonate.  This setup allows the presence of microwave photons in the resonator to influence the behavior of the current in the wire and vice versa.  In a transmon, microwave photons are used to control the current.  But there are other companies that have hardware that does the reverse, controlling the state of the photons by altering the current. Dual-rail qubits use two of these systems linked together, allowing photons to move from the resonator to the other.  Using the superconducting loops, it's possible to control the probability that a photon will end up in the left or right resonator.  The actual location of the photon will remain unknown until it's measured, allowing the system as a whole to hold a single bit of quantum information—a qubit. 

This has an obvious disadvantage: You have to build twice as much hardware for the same number of qubits.  So why bother? Because the vast majority of errors involve the loss of the photon, and that's easily detected.  "It's about 90 percent or more [of the errors]," said Quantum Circuits' Andrei Petrenko.  "So it's a huge advantage that we have with photon loss over other errors.  And that's actually what makes the error correction a lot more efficient: The fact that photon losses are by far the dominant error. "

Petrenko said that, without doing a measurement that would disrupt the storage of the qubit, it's possible to determine if there is an odd number of photons in the hardware.  If that isn't the case, you know an error has occurred—most likely a photon loss (gains of photons are rare but do occur).  For simple algorithms, this would be a signal to simply start over. 

But it does not eliminate the need for error correction if we want to do more complex computations that can't make it to completion without encountering an error.  There's still the remaining 10 percent of errors, which are primarily something called a phase flip that is distinct to quantum systems.  Bit flips are even more rare in dual-rail setups.  Finally, simply knowing that a photon was lost doesn't tell you everything you need to know to fix the problem; error-correction measurements of other parts of the logical qubit are still needed to fix any problems. 

In fact, the initial hardware that's being made available is too small to even approach useful computations.  Instead, Quantum Circuits chose to link eight qubits with nearest-neighbor connections in order to allow it to host a single logical qubit that enables error correction.  Put differently: this machine is meant to enable people to learn how to use the unique features of dual-rail qubits to improve error correction. One consequence of having this distinctive hardware is that the software stack that controls operations needs to take advantage of its error detection capabilities.  None of the other hardware on the market can be directly queried to determine whether it has encountered an error.  So, Quantum Circuits has had to develop its own software stack to allow users to actually benefit from dual-rail qubits.  Petrenko said that the company also chose to provide access to its hardware via its own cloud service because it wanted to connect directly with the early adopters in order to better understand their needs and expectations. 

Numbers or noise?
Given that a number of companies have already released multiple revisions of their quantum hardware and have scaled them into hundreds of individual qubits, it may seem a bit strange to see a company enter the market now with a machine that has just a handful of qubits.  But amazingly, Quantum Circuits isn't alone in planning a relatively late entry into the market with hardware that only hosts a few qubits. 

Having talked with several of them, there is a logic to what they're doing.  What follows is my attempt to convey that logic in a general form, without focusing on any single company's case. 

Everyone agrees that the future of quantum computation is error correction, which requires linking together multiple hardware qubits into a single unit termed a logical qubit.  To get really robust, error-free performance, you have two choices.  One is to devote lots of hardware qubits to the logical qubit, so you can handle multiple errors at once.  Or you can lower the error rate of the hardware, so that you can get a logical qubit with equivalent performance while using fewer hardware qubits.  (The two options aren't mutually exclusive, and everyone will need to do a bit of both. )The two options pose very different challenges.  Improving the hardware error rate means diving into the physics of individual qubits and the hardware that controls them.  In other words, getting lasers that have fewer of the inevitable fluctuations in frequency and energy.  Or figuring out how to manufacture loops of superconducting wire with fewer defects or handle stray charges on the surface of electronics.  These are relatively hard problems. 

By contrast, scaling qubit count largely involves being able to consistently do something you already know how to do.  So, if you already know how to make good superconducting wire, you simply need to make a few thousand instances of that wire instead of a few dozen.  The electronics that will trap an atom can be made in a way that will make it easier to make them thousands of times.  These are mostly engineering problems, and generally of similar complexity to problems we've already solved to make the electronics revolution happen. 

In other words, within limits, scaling is a much easier problem to solve than errors.  It's still going to be extremely difficult to get the millions of hardware qubits we'd need to error correct complex algorithms on today's hardware.  But if we can get the error rate down a bit, we can use smaller logical qubits and might only need 10,000 hardware qubits, which will be more approachable. 

Errors first
And there's evidence that even the early entries in quantum computing have reasoned the same way.  Google has been working iterations of the same chip design since its 2019 quantum supremacy announcement, focusing on understanding the errors that occur on improved versions of that chip.  IBM made hitting the 1,000 qubit mark a major goal but has since been focused on reducing the error rate in smaller processors.  Someone at a quantum computing startup once told us it would be trivial to trap more atoms in its hardware and boost the qubit count, but there wasn't much point in doing so given the error rates of the qubits on the then-current generation machine. The new companies entering this market now are making the argument that they have a technology that will either radically reduce the error rate or make handling the errors that do occur much easier.  Quantum Circuits clearly falls into the latter category, as dual-rail qubits are entirely about making the most common form of error trivial to detect.  The former category includes companies like Oxford Ionics, which has indicated it can perform single-qubit gates with a fidelity of over 99. 9991 percent.  Or Alice & Bob, which stores qubits in the behavior of multiple photons in a single resonance cavity, making them very robust to the loss of individual photons. 

These companies are betting that they have distinct technology that will let them handle error rate issues more effectively than established players.  That will lower the total scaling they need to do, and scaling will be an easier problem overall—and one that they may already have the pieces in place to handle.  Quantum Circuits' Petrenko, for example, told Ars, "I think that we're at the point where we've gone through a number of iterations of this qubit architecture where we've de-risked a number of the engineering roadblocks. " And Oxford Ionics told us that if they could make the electronics they use to trap ions in their hardware once, it would be easy to mass manufacture them. 

None of this should imply that these companies will have it easy compared to a startup that already has experience with both reducing errors and scaling, or a giant like Google or IBM that has the resources to do both.  But it does explain why, even at this stage in quantum computing's development, we're still seeing startups enter the field. Study: Warming has accelerated due to the Earth absorbing more sunlight
If it's a trend, then future warming will be at the high end of estimates. 2023 was always going to be a hot year, given that warmer El Niño conditions were superimposed on the long-term trend of climate change driven by our greenhouse gas emissions.  But it's not clear anybody was expecting the striking string of hot months that allowed the year to easily eclipse any previous year on record.  As the warmth has continued at record levels even after the El Niño faded, it's an event that seems to demand an explanation. 

On Thursday, a group of German scientists—Helge Goessling, Thomas Rackow, and Thomas Jung—released a paper that attempts to provide one.  They present data that suggests the Earth is absorbing more incoming sunlight than it has in the past, largely due to reduced cloud cover. 

Balancing the numbers on radiation
Years with strong El Niño conditions tend to break records.  But the 2023 El Niño was relatively mild.  The effects of the phenomenon are also directly felt in the tropical Pacific, yet ocean temperatures set records in the Atlantic and contributed to a massive retreat in ice near Antarctica.  So, there are clearly limits to what can be attributed to El Niño.  Other influences that have been considered include the injection of water vapor into the stratosphere by the Hunga Tonga eruption, and a reduction in sulfur emissions due to new rules governing international shipping.  2023 also corresponds to a peak in the most recent solar cycle. 

But it is estimated that all three factors combined add up to, at most, 0. 1 Kelvin of warming on top of the El Niño, which leaves an estimated 0. 2 Kelvin of additional warming unaccounted for.  The authors trace the difference back to an energy imbalance measured at the top of the atmosphere. The concept of an atmospheric energy imbalance is pretty straightforward: We can measure both the amount of energy the Earth receives from the Sun and how much energy it radiates back into space.  Any difference between the two results in a net energy imbalance that's either absorbed by or extracted from the ocean/atmosphere system.  And we've been tracking it via satellite for a while now as rising greenhouse gas levels have gradually increased the imbalance. 

But greenhouse gases aren't the only thing having an effect.  For example, the imbalance has also increased in the Arctic due to the loss of snow cover and retreat of sea ice.  The dark ground and ocean absorb more solar energy compared to the white material that had previously been exposed to the sunlight.  Not all of this is felt directly, however, as a lot of the areas where it's happening are frequently covered by clouds. 

Nevertheless, the loss of snow and ice has caused the Earth's reflectivity, termed its albedo, to decline since the 1970s, enhancing the warming a bit. 

Vanishing clouds
The new paper finds that the energy imbalance set a new high in 2023, with a record amount of energy being absorbed by the ocean/atmosphere system.  This wasn't accompanied by a drop in infrared emissions from the Earth, suggesting it wasn't due to greenhouse gases, which trap heat by absorbing this radiation.  Instead, it seems to be due to decreased reflection of incoming sunlight by the Earth. 

While there was a general trend in that direction, the planet set a new record low for albedo in 2023.  Using two different data sets, the teams identify the areas most effected by this, and they're not at the poles, indicating loss of snow and ice are unlikely to be the cause.  Instead, the key contributor appears to be the loss of low-level clouds.  "The cloud-related albedo reduction is apparently largely due to a pronounced decline of low-level clouds over the northern mid-latitude and tropical oceans, in particular the Atlantic," the researchers say. The drop in low-level clouds had been averaging about 1. 3 percent per decade.  2023 saw a slightly larger drop occur in just one year.  The researchers calculate that this could account for all of the discrepancy between the unusually hot 2023 conditions and the known factors that could have influenced global temperatures. 

So, what could be causing the clouds to go away? The researchers list three potential factors.  One is simply the variability of the climate system, meaning 2023 might have just been an extremely unusual year, and things will revert to trends in the ensuing years.  The second is the impact of aerosols, which both we and natural processes emit in copious quantities.  These can help seed clouds, so a reduction of aerosols (driven by things like pollution control measures) could potentially account for this effect. 

The most concerning potential explanation, however, is that there may be a feedback relationship between rising temperatures and low-level clouds.  Meaning that, as the Earth warms, the clouds become sparse, enhancing the warming further.  That would be bad news for our future climate, because it suggests that the lower range of warming estimates would have to be adjusted upward to account for it. 

"If the cloud-related albedo decline was caused not solely by internal variability, the 2023 extra heat may be here to stay and Earth’s climate sensitivity may be closer to the upper range of current estimates," Goessling, Rackow, and Jung conclude.  And that adds a bit of urgency to the need to understand why 2023 and 2024 have been so exceptionally warm. 