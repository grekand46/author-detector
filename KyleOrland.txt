The return of Steam Machines? Valve rolls out new “Powered by SteamOS” branding. 
New branding guidelines suggest official third-party hardware support is imminent. 
Longtime Valve watchers likely remember Steam Machines, the company's aborted, pre-Steam Deck attempt at crafting a line of third-party gaming PC hardware based around an early verison of its Linux-based SteamOS.  Now, there are strong signs that Valve is on the verge of launching a similar third-party hardware branding effort under the "Powered by SteamOS" label. 

The newest sign of those plans come via newly updated branding guidelines posted by Valve on Wednesday (as noticed by the trackers at SteamDB).  That update includes the first appearance of a new "Powered by SteamOS" logo intended "for hardware running the SteamOS operating system, implemented in close collaboration with Valve. "

The document goes on to clarify that the new Powered by SteamOS logo "indicates that a hardware device will run the SteamOS and boot into SteamOS upon powering on the device. " That's distinct from the licensed branding for merely "Steam Compatible" devices, which include "non-Valve input peripherals" that have been reviewed by Valve to work with Steam. 

The new guidelines replace an older set of branding guidelines, last revised in late 2017, that included detailed instructions for how to use the old "Steam Machines" name and logo on third-party hardware.  That branding has been functionally defunct for years, making Valve's apparent need to suddenly update it more than a little suspect. Steam Machines 2. 0?
While there hasn't been any wider official rollout for the Powered by SteamOS program yet, any move by Valve to offer officially licensed SteamOS compatibility for third-party hardware wouldn't come as a complete surprise.  Valve's Lawrence Yang has been saying since 2022 that "we’re excited to see people make their own SteamOS machines. " And last November, Yang told PC Gamer that SteamOS would be made "more available to other handhelds with a similar gamepad-style controller" on a rough time frame of "soon. "

In August, after a SteamOS beta update suggested SteamOS might be coming to Asus' Windows-based ROG Ally handheld, Yang told The Verge that the Valve hardware team "is continuing to work on adding support for additional handhelds on SteamOS. "

As we've waited for that official support to materialize—and for a long-promised general public distribution of SteamOS 3 on PCs— fans have had to get a bit creative to get the Linux-powered, gaming-focused OS onto their devices.  Earlier this year, Ayaneo announced its Next Lite handheld would ship with HoloISO, an Arch Linux fork that seeks to "provide a close-to-official SteamOS experience" without Valve's official support. 

In 2015, the limited and underperforming software support for early SteamOS made Steam Machines a pretty poor alternative to Windows-powered gaming rigs.  Today, the launch of the Steam Deck and the wide implementation of Proton-powered cross-compatibility has made modern SteamOS a much more appealing alternative to a costly Windows license for hardware OEMs.  Here's hoping more hardware makers get the opportunity to make official use of that alternative very soon. Are LLMs capable of non-verbal reasoning?
Processing in the "latent space" could help AI with tricky logical questions. Large language models have found great success so far by using their transformer architecture to effectively predict the next words (i. e. , language tokens) needed to respond to queries.  When it comes to complex reasoning tasks that require abstract logic, though, some researchers have found that interpreting everything through this kind of "language space" can start to cause some problems, even for modern "reasoning" models. 

Now, researchers are trying to work around these problems by crafting models that can work out potential logical solutions completely in "latent space"—the hidden computational layer just before the transformer generates language.  While this approach doesn't cause a sea change in an LLM's reasoning capabilities, it does show distinct improvements in accuracy for certain types of logical problems and shows some interesting directions for new research. 

Wait, what space?
Modern reasoning models like ChatGPT's o1 tend to work by generating a "chain of thought. " Each step of the logical process in these models is expressed as a sequence of natural language word tokens that are fed back through the model. 

In a new paper, researchers at Meta's Fundamental AI Research team (FAIR) and UC San Diego identify this reliance on natural language and "word tokens" as a "fundamental constraint" for these reasoning models.  That's because the successful completion of reasoning tasks often requires complex planning on specific critical tokens to figure out the right logical path from a number of options. In current chain-of-thought models, though, word tokens are often generated for "textual coherence" and "fluency" while "contributing little to the actual reasoning process," the researchers write.  Instead, they suggest, "it would be ideal for LLMs to have the freedom to reason without any language constraints and then translate their findings into language only when necessary. "To achieve that "ideal," the researchers describe a method for "Training Large Language Models to Reason in a Continuous Latent Space," as the paper's title puts it.  That "latent space" is essentially made up of the "hidden" set of intermediate token weightings that the model contains just before the transformer generates a human-readable natural language version of that internal state. 
In the researchers' COCONUT model (for Chain Of CONtinUous Thought), those kinds of hidden states are encoded as "latent thoughts" that replace the individual written steps in a logical sequence both during training and when processing a query.  This avoids the need to convert to and from natural language for each step and "frees the reasoning from being within the language space," the researchers write, leading to an optimized reasoning path that they term a "continuous thought. "

Being more breadth-minded
While doing logical processing in the latent space has some benefits for model efficiency, the more important finding is that this kind of model can "encode multiple potential next steps simultaneously. " Rather than having to pursue individual logical options fully and one by one (in a "greedy" sort of process), staying in the "latent space" allows for a kind of instant backtracking that the researchers compare to a breadth-first-search through a graph. 

This emergent, simultaneous processing property comes through in testing even though the model isn't explicitly trained to do so, the researchers write.  "While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions," they write. That kind of multi-path reasoning didn't really improve COCONUT's accuracy over traditional chain-of-thought models on relatively straightforward tests of math reasoning (GSM8K) or general reasoning (ProntoQA).  But the researchers found the model did comparatively well on a randomly generated set of ProntoQA-style queries involving complex and winding sets of logical conditions (e. g. , "every apple is a fruit, every fruit is food, etc. "). 

For these tasks, standard chain-of-thought reasoning models would often get stuck down dead-end paths of inference or even hallucinate completely made-up rules when trying to resolve the logical chain.  Previous research has also shown that the "verbalized" logical steps output by these chain-of-thought models "may actually utilize a different latent reasoning process" than the one being shared. 

This new research joins a growing body of research seeking to understand and exploit the way large language models work at the level of their underlying neural networks.  And while that kind of research hasn't led to a huge breakthrough just yet, the researchers conclude that models pre-trained with these kinds of "continuous thoughts" from the get-go could "enable models to generalize more effectively across a wider range of reasoning scenarios. "
Itch. io platform briefly goes down due to “AI-driven” anti-phishing report
Domain registrar failed to respond after offending content was taken down. 

Popular indie game platform itch. io says its domain was briefly taken down for a few hours Monday morning thanks to an "AI-driven" phishing report spurred by the company behind Funko Pop figures. 

Itch. io management posted about the domain takedown on social media overnight, complaining of a chain of events that started because "Funko of 'Funko Pop'. . .  use some trash 'AI Powered' Brand Protection Software called BrandShield that created some bogus Phishing report to our registrar, iwantmyname, who ignored our response and just disabled the domain," the post said. 

In a Hacker News comment, Itch. io founder Leaf "Leafo" Cohran said that the BrandShield complaint seems to have originated from a single itch. io user who "made a fan page for an existing Funko Pop video game (Funko Fusion), with links to the official site and screenshots of the game. " That led to independent reports to Itch's host and registrar of "fraud and phishing" a few days ago. 

While Cochran says the offending page was taken down immediately after the complaints were filed, he suspects the initial complaint meant "our registrar's automated system likely kicked to disable the domain since no one read our confirmation of removal. "The itch. io domain was back up and running by 7 am Eastern, according to media reports, "after the registrant finally responded to our notice and took appropriate action to resolve the issue. " Users could access the site throughout if they typed the itch. io IP address into their web browser directly. 

Too strong a shield?
BrandShield's website describes it as a service that "detects and hunts online trademark infringement, counterfeit sales, and brand abuse across multiple platforms. ” The company claims to have multiple Fortune 500 and FTSE100 companies on its client list. 

In its own series of social media posts, BrandShield said its "AI-driven platform" had identified "an abuse of Funko. . .  from an itch. io subdomain. " The takedown request it filed was focused on that subdomain, not the entirety of itch. io, BrandShield said. 

"The temporary takedown of the website was a decision made by the service providers, not BrandShield or Funko. "

The whole affair highlights how the delicate web of domain registrars and DNS servers can remain a key failure point for web-based businesses.  Back in May, we saw how the desyncing of a single DNS root server could cause problems across the entire Internet.  And in 2012, the hacking collective Anonymous highlighted the potential for a coordinated attack to take down the entire DNS system. 
Google’s Genie 2 “world model” reveal leaves more questions than answers
Long-term persistence, real-time interactions remain huge hurdles for AI worlds. In March, Google showed off its first Genie AI model.  After training on thousands of hours of 2D run-and-jump video games, the model could generate halfway-passable interactive impressions of those games based on generic images or text descriptions. 

Nine months later, this week's reveal of the Genie 2 model expands that idea into the realm of fully 3D worlds, complete with controllable third- or first-person avatars.  Google's announcement talks up Genie 2's role as a "foundational world model" that can create a fully interactive internal representation of a virtual environment.  That could allow AI agents to train themselves in synthetic but realistic environments, Google says, forming an important stepping stone on the way to artificial general intelligence. 

But while Genie 2 shows just how much progress Google's Deepmind team has achieved in the last nine months, the limited public information about the model thus far leaves a lot of questions about how close we are to these foundational model worlds being useful for anything but some short but sweet demos. 

How long is your memory?
Much like the original 2D Genie model, Genie 2 starts from a single image or text description and then generates subsequent frames of video based on both the previous frames and fresh input from the user (such as a movement direction or "jump").  Google says it trained on a "large-scale video dataset" to achieve this, but it doesn't say just how much training data was necessary compared to the 30,000 hours of footage used to train the first Genie. 

Short GIF demos on the Google DeepMind promotional page show Genie 2 being used to animate avatars ranging from wooden puppets to intricate robots to a boat on the water.  Simple interactions shown in those GIFs demonstrate those avatars busting balloons, climbing ladders, and shooting exploding barrels without any explicit game engine describing those interactions. Perhaps the biggest advance claimed by Google here is Genie 2's "long horizon memory. " This feature allows the model to remember parts of the world as they come out of view and then render them accurately as they come back into the frame based on avatar movement.  This kind of persistence has proven to be a persistent problem for video-generation models like Sora, which OpenAI said in February "do[es] not always yield correct changes in object state" and can develop "incoherencies. . .  in long duration samples. "The "long horizon" part of "long horizon memory" is perhaps a little overzealous here, though, as Genie 2 only "maintains a consistent world for up to a minute," with "the majority of examples shown lasting [10 to 20 seconds]. " Those are definitely impressive time horizons in the world of AI video consistency, but it's pretty far from what you'd expect from any other real-time game engine.  Imagine entering a town in a Skyrim-style RPG, then coming back five minutes later to find that the game engine had forgotten what that town looks like and generated a completely different town from scratch instead. 

What are we prototyping, exactly?
Perhaps for this reason, Google suggests Genie 2 as it stands is less useful for creating a complete game experience and more to "rapidly prototype diverse interactive experiences" or to turn "concept art and drawings. . .  into fully interactive environments. "

The ability to transform static "concept art" into lightly interactive "concept videos" could definitely be useful for visual artists brainstorming ideas for new game worlds.  However, these kinds of AI-generated samples might be less useful for prototyping actual game designs that go beyond the visual. On Bluesky, British game designer Sam Barlow (Silent Hill: Shattered Memories, Her Story) points out how game designers often use a process called whiteboxing to lay out the structure of a game world as simple white boxes well before the artistic vision is set.  The idea, he says, is to "prove out and create a gameplay-first version of the game that we can lock so that art can come in and add expensive visuals to the structure.  We build in lo-fi because it allows us to focus on these issues and iterate on them cheaply before we are too far gone to correct. "

Generating elaborate visual worlds using a model like Genie 2 before designing that underlying structure feels a bit like putting the cart before the horse.  The process almost seems designed to generate generic, "asset flip"-style worlds with AI-generated visuals papered over generic interactions and architecture. 

As podcaster Ryan Zhao put it on Bluesky, "The design process has gone wrong when what you need to prototype is 'what if there was a space. '"

Gotta go fast
When Google revealed the first version of Genie earlier this year, it also released a detailed research paper outlining the specific steps taken behind the scenes to train the model and how that model generated interactive videos.  They haven't done the same for a research paper detailing Genie 2's process, leaving us guessing at some important details. 

One of the most important of these details is model speed.  The first Genie model generated its world at roughly one frame per second, a rate that was orders of magnitude slower than would be tolerably playable in real time.  For Genie 2, Google only says that "the samples in this blog post are generated by an undistilled base model, to show what is possible.  We can play a distilled version in real-time with a reduction in quality of the outputs. "

Reading between the lines, it sounds like the full version of Genie 2 operates at something well below the real-time interactions implied by those flashy GIFs.  It's unclear how much "reduction in quality" is necessary to get a diluted version of the model to real-time controls, but given the lack of examples presented by Google, we have to assume that reduction is significant. Real-time, interactive AI video generation isn't exactly a pipe dream.  Earlier this year, AI model maker Decart and hardware maker Etched published the Oasis model, showing off a human-controllable, AI-generated video clone of Minecraft that runs at a full 20 frames per second.  However, that 500 million parameter model was trained on millions of hours of footage of a single, relatively simple game, and focused exclusively on the limited set of actions and environmental designs inherent to that game. 

When Oasis launched, its creators fully admitted the model "struggles with domain generalization," showing how "realistic" starting scenes had to be reduced to simplistic Minecraft blocks to achieve good results.  And even with those limitations, it's not hard to find footage of Oasis degenerating into horrifying nightmare fuel after just a few minutes of play. We can already see similar signs of degeneration in the extremely short GIFs shared by the Genie team, such as an avatar's dream-like fuzz during high-speed movement or NPCs that quickly fade into undifferentiated blobs at a short distance.  That's not a great sign for a model whose "long memory horizon" is supposed to be a key feature. 

A learning crèche for other AI agents?
Genie 2 seems to be using individual game frames as the basis for the animations in its model.  But it also seems able to infer some basic information about the objects in those frames and craft interactions with those objects in the way a game engine might. 

Google's blog post shows how a SIMA agent inserted into a Genie 2 scene can follow simple instructions like "enter the red door" or "enter the blue door," controlling the avatar via simple keyboard and mouse inputs.  That could potentially make Genie 2 environment a great test bed for AI agents in various synthetic worlds. 

Google claims rather grandiosely that Genie 2 puts it on "the path to solving a structural problem of training embodied agents safely while achieving the breadth and generality required to progress towards [artificial general intelligence]. " Whether or not that ends up being true, recent research shows that agent learning gained from foundational models can be effectively applied to real-world robotics. 

Using this kind of AI model to create worlds for other AI models to learn in might be the ultimate use case for this kind of technology.  But when it comes to the dream of an AI model that can create generic 3D worlds that a human player could explore in real time, we might not be as close as it seems. OpenAI is at war with its own Sora video testers following brief public leak
Group behind stunt says they're being used for "unpaid R&D" and "art washing. "OpenAI has cut off testing access to its Sora video generation platform after a group of artists briefly shared their own early access in a publicly usable webpage Tuesday.  The group, going by the moniker PR Puppets, claims the stunt was a protest against being asked to work as unpaid R&D and bug testers while participating in "art washing" of AI tools.  But OpenAI says participation in the early alpha test is voluntary and has no requirements that testers provide feedback. 

PR Puppets posted its "Generate with Sora" access point to Hugging Face at about 8:30 Eastern time Tuesday morning, according to Git commit logs.  Quickly, AI experts on social media noticed the posting and confirmed that the page connected to endpoints on OpenAI's actual Sora API and hosting on a videos. openai. com domain, presumably with authentication tokens provided to testers by OpenAI itself. 

That access was revoked within hours, but not before plenty of eager followers managed to generate their own videos and share them on social media.  An OpenAI spokesperson told The Washington Post the company is temporarily pausing all test access to Sora to evaluate the situation.  Other users dug into the code to discover hints of different modes and "styles" that might be in development for Sora. 

Who's using whom?
In an open letter addressed to their "Corporate AI Overlords," the PR Puppets group said that it was protesting on behalf of roughly 300 artists that had been provided early access to Sora only to "provide unpaid labor through bug testing, feedback and experimental work for the program for a $150B valued company [OpenAI]. " The group claimed only a select few of those artists will see their work pay off in the form of wider screening for their Sora-created films and complained that OpenAI requires approval before any Sora alpha output can be shared publicly. "We are not against the use of AI technology as a tool for the arts (if we were, we probably wouldn't have been invited to this program)," PR Puppets writes.  "What we don't agree with is how this artist program has been rolled out and how the tool is shaping up ahead of a possible public release.  We are sharing this to the world in the hopes that OpenAI becomes more open, more artist friendly and supports the arts beyond PR stunts. "

In a statement provided to Ars Technica, an OpenAI spokesperson noted that "Sora is still in research preview, and we’re working to balance creativity with robust safety measures for broader use.  Hundreds of artists in our alpha have shaped Sora’s development, helping prioritize new features and safeguards.  Participation is voluntary, with no obligation to provide feedback or use the tool. "

Throughout the day Tuesday, PR Puppets updated its open letter with signatures from 16 people and groups listed as "sora-alpha-artists. " But a source with knowledge of OpenAI's testing program told Ars that only a couple of those artists were actually part of the alpha testing group and that those artists were asked to refrain from sharing confidential details during Sora's development. 

PR Puppets also later linked to a public petition encouraging others to sign on to the same message shared in their open letter.  Artists Memo Akten, Jake Elwes, and CROSSLUCID, who are also listed as "sora-alpha-artists," were among the first to sign that public petition. 

When can we get in?Sora made a huge splash when OpenAI first teased its video-generation capabilities in February, before shopping the tech around Hollywood and using it in a public advertisement for Toys R Us.  Since then, though, publicly accessible video generators like Minimax and announcements of in-development competitors from Google and Meta have stolen some of Sora's initial thunder. 

Previous OpenAI CTO Mira Murati told The Wall Street Journal in March that it planned to release Sora publicly by the end of the year.  But CPO Kevin Weil said in a recent Reddit AMA that the platform's deployment has been delayed by the "need to perfect the model, need to get safety/impersonation/other things right, and need to scale compute!"